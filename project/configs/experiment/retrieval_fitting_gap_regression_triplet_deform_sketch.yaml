# @package _global_
# to execute this experiment run:
# python train.py +experiment=exp_example_simple

defaults:
    - override /trainer: default_trainer.yaml              # choose trainer from 'configs/trainer/' folder
    - override /model: kp_deformer.yaml                 # choose model from 'configs/model/' folder
    - override /callbacks: null                     # choose callback set from 'configs/callbacks/' folder

seed: 12345

datamodule:
    _target_: src.datamodules.SketchyVR_datamodule.SketchyVRDataModule
    dataset_loader: SketchyVR_original
    batch_size: 10

model:
    _target_: src.models.deformer_retrieval.Deformer_Retrieval_fitting_gap
    num_structure_points: 12
    lr: 0.01
    vis: True
    deformer_ckpt: logs/experiments/deformer_cage_sh2sh_template/checkpoints/last.ckpt
    loss_type: regression_triplet
    neg_ratio: 1.
    margin: 0.3
    margin_max: 1.2
    softmax_weight: 1.
    step_size: 80
    regression_triplet_loss: regression_triplet_loss_v7
    lambda_deform_sketch: 1.0
    deform_sketch_loss: deform_sketch_loss_v1
    use_sketch_deformer: True

trainer:
    max_epochs: 300

callbacks:
    model_checkpoint:
        _target_: src.callbacks.custom_callbacks.SelfModelCheckpoint
        monitor: "val_acc"      # name of the logged metric which determines when model is improving
        save_top_k: 1           # save k best models (determined by above metric)
        save_last: True         # additionaly always save model from last epoch
        mode: "max"             # can be "max" or "min"
        # every_n_val_epochs: 1
        dirpath: checkpoints
        filename: "{val_acc:.3f}-{epoch}"
    
    LearningRateMonitor:
        _target_: pytorch_lightning.callbacks.lr_monitor.LearningRateMonitor

# resume_training: True
# rank_by: encoder