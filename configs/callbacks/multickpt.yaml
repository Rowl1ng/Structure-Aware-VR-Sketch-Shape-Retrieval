model_checkpoint1:
    _target_: src.callbacks.custom_callbacks.SelfModelCheckpoint
    monitor: "val_CD_d@1"      # name of the logged metric which determines when model is improving
    save_top_k: 1           # save k best models (determined by above metric)
    save_last: True         # additionaly always save model from last epoch
    mode: "min"             # can be "max" or "min"
    dirpath: checkpoints
    filename: "{val_CD_d@1:.3f}-{epoch}"
model_checkpoint2:
    _target_: src.callbacks.custom_callbacks.SelfModelCheckpoint
    monitor: "val_bi_CD_d@1"      # name of the logged metric which determines when model is improving
    save_top_k: 1           # save k best models (determined by above metric)
    save_last: False         # additionaly always save model from last epoch
    mode: "min"             # can be "max" or "min"
    dirpath: checkpoints
    filename: "{val_bi_CD_d@1:.3f}-{epoch}"   
model_checkpoint3:
    _target_: src.callbacks.custom_callbacks.SelfModelCheckpoint
    monitor: "val_CD_d@5"      # name of the logged metric which determines when model is improving
    save_top_k: 1           # save k best models (determined by above metric)
    save_last: False         # additionaly always save model from last epoch
    mode: "min"             # can be "max" or "min"
    dirpath: checkpoints
    filename: "{val_CD_d@5:.3f}-{epoch}"  
model_checkpoint4:
    _target_: src.callbacks.custom_callbacks.SelfModelCheckpoint
    monitor: "val_bi_CD_d@5"      # name of the logged metric which determines when model is improving
    save_top_k: 1           # save k best models (determined by above metric)
    save_last: False         # additionaly always save model from last epoch
    mode: "min"             # can be "max" or "min"
    dirpath: checkpoints
    filename: "{val_bi_CD_d@5:.3f}-{epoch}"  
LearningRateMonitor:
    _target_: pytorch_lightning.callbacks.lr_monitor.LearningRateMonitor
